#
# Example slurm.conf file generated by DeepOps. 
#
# Slurm provides configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
# See the slurm.conf man page for more information.

# Define a name for the cluster
ClusterName={{ slurm_cluster_name }}

# Configure the controllers
{% if slurm_enable_ha %}
{% for node_name in groups['slurm-master'] %}
SlurmctldHost={{ node_name }}
{% endfor %}
StateSaveLocation={{ slurm_ha_state_save_location }}
{% else %}
SlurmctldHost={{ groups['slurm-master'][0] }}
StateSaveLocation=/var/spool/slurm/ctld
{% endif %}

# Basic configuration
SlurmUser={{ slurm_username }}
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir={{ slurm_install_prefix }}/lib/slurm
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PropagateResourceLimitsExcept=MEMLOCK

# Basic job behavior
ReturnToService={{ slurm_return_to_service }}
RebootProgram="{{ slurm_reboot_program }}"
ResumeTimeout={{ slurm_resume_timeout }}

{% if slurm_include_pmix %}
# Use PMIX as our default MPI configuration
MpiDefault=pmix
{% endif %}

# Prolog/epilog config
{% if slurm_contain_ssh is defined %}
PrologFlags=Alloc,Serial,Contain
{% else %}
PrologFlags=Alloc,Serial
{% endif %}
{% if slurm_enable_prolog_epilog %}
Prolog={{ slurm_config_dir }}/prolog.sh
Epilog={{ slurm_config_dir }}/epilog.sh
{% endif %}
#SrunProlog=
#SrunEpilog=
TaskProlog={{ slurm_config_dir }}/task_prolog.sh
#TaskEpilog=

{% if slurm_health_check_program is defined %}
# Health checking
HealthCheckProgram={{ slurm_health_check_program }}
HealthCheckInterval=300
HealthCheckNodeState=IDLE
{% endif %}

# Mail program to use
MailProg=/usr/bin/s-nail

TaskPlugin=affinity,cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=

# TIMERS
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
UnkillableStepTimeOut=180

# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0

# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=

# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
{% if slurm_manage_gpus %}
AccountingStorageTRES=gres/gpu
{% endif %}
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ groups["slurm-master"][0] }}
#AccountingStorageLoc=
AccountingStorageEnforce=associations,limits,qos
AccountingStorageUser={{ slurm_db_username }}
AccountingStoragePass=/var/run/munge/munge.socket.2

# COMPUTE NODES
{% if slurm_manage_gpus %}
GresTypes=gpu
{% endif %}
{% for node_name in groups['slurm-node'] %}
{% set memory =  hostvars[node_name]["ansible_local"]["memory"] -%}
{% set cpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["cpu_topology"] -%}
{% set gpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["gpu_topology"] -%}
    NodeName={{ node_name }}{{ " " -}}
    {% if slurm_manage_gpus %} {%- if gpu_topology|count %} Gres=gpu:{{ gpu_topology|count }} {% endif -%} {% endif %}
    CPUs={{ cpu_topology.logical_cpus|int }}{{ " " -}}
    Sockets={{ cpu_topology.sockets|int }}{{ " " -}}
    CoresPerSocket={{ cpu_topology.cores_per_socket|int }}{{ " " -}}
    ThreadsPerCore={{ cpu_topology.threads_per_core }}{{ " " -}}
    Procs={{ cpu_topology.sockets|int * cpu_topology.cores_per_socket|int }}{{ " " -}}
    RealMemory={{ memory.total_mb|int }}{{ " " -}}
    State=UNKNOWN
{% endfor %}

# TODO create this as a fact
{% set partition_nodes = {} %}
{% for node_name in groups["slurm-node"] %}
    {% set partition_name = hostvars[node_name]["slurm_partition_name"] -%}
    {% if partition_name not in partition_nodes %}
       {% set _=partition_nodes.__setitem__(partition_name, []) %}
    {% endif %}
    {% set current_nodes=partition_nodes.__getitem__(partition_name) %}
    {% set _=current_nodes.append(node_name) %}
{% endfor %}

# Partition table
{% for partition_name in partition_nodes %}
{% set nodenames=partition_nodes[partition_name]|join(",") -%}
{% set slurm_oversubscribe = (slurm_manage_gpus == true)|ternary('NO', 'EXCLUSIVE') %}
{% set memory_per_partition=partition_settings[partition_name]["slurm_def_mem_per_cpu"] %}
{% set cpu_per_gpu=partition_settings[partition_name]["slurm_def_cpu_per_gpu"] %}
{% set max_job_timelimit=partition_settings[partition_name]["slurm_max_job_timelimit"] %}
{% set default_partition=partition_settings[partition_name]["default"] %}
{% set is_default=(default_partition == true)|ternary('YES', 'NO') %}
{% set slurm_allow_qos=partition_settings[partition_name]["slurm_allow_qos"] %}

# {{ memory_per_partition }} {{ max_job_timelimit }} {{ is_default }}
PartitionName={{ partition_name }} Nodes={{ nodenames }} State=UP Default={{ is_default }} OverSubscribe={{ slurm_oversubscribe }} {%- if slurm_max_job_timelimit is defined %} MaxTime={{ max_job_timelimit }} {%- else %} MaxTime=INFINITE {% endif -%} {%- if slurm_default_job_timelimit is defined %} DefaultTime={{ slurm_default_job_timelimit }} {% endif -%} {%- if slurm_grace_time is defined %} GraceTime={{ slurm_grace_time }} {%- else %} GraceTime=0 {% endif -%} {%- if memory_per_partition is defined %} DefMemPerCPU={{ memory_per_partition }} {%- else %}{% endif %} {%- if cpu_per_gpu is defined %} DefCpuPerGPU={{ cpu_per_gpu }} {%- else %}{% endif %} AllowQos={{ slurm_allow_qos }}
{% endfor %}

